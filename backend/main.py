import torchvision
try:
    import torchvision.transforms.functional_tensor
except ImportError:
    import torchvision.transforms.functional as F
    import sys
    sys.modules["torchvision.transforms.functional_tensor"] = F
# ---------------------------------------

from fastapi import FastAPI, HTTPException, File, UploadFile, Form
# ðŸ‘‡ CORS ë¯¸ë“¤ì›¨ì–´ ìž„í¬íŠ¸
from fastapi.middleware.cors import CORSMiddleware 
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import torch
import numpy as np
import cv2
import io
import base64
import asyncio
import re
import textwrap
import requests
from typing import Optional, List, Tuple, Union, Dict
from PIL import Image, ImageDraw, ImageFont, ImageOps, ImageColor

# â­ï¸ HEIC í¬ë§· ì§€ì›
try:
    from pillow_heif import register_heif_opener
    register_heif_opener()
    print("âœ… HEIC Image Support Enabled.")
except ImportError:
    print("âš ï¸ pillow-heif not found. HEIC images might cause errors.")

import databases
import sqlalchemy
from rembg import new_session, remove 
from realesrgan import RealESRGANer 
from basicsr.archs.rrdbnet_arch import RRDBNet 
import openai 
import httpx 

load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
IMAGE_BASE_PATH = os.getenv("IMAGE_BASE_PATH", "/inday_fileinfo/img")
SITE_BASE_URL = os.getenv("SITE_BASE_URL", "https://www.pimfyvirus.com")

database = databases.Database(DATABASE_URL) if DATABASE_URL else None
metadata = sqlalchemy.MetaData()

# --- DB í…Œì´ë¸” ì •ì˜ ---
dogs_table = sqlalchemy.Table(
    "homeprotection", metadata,
    sqlalchemy.Column("uid", sqlalchemy.Integer, primary_key=True),
    sqlalchemy.Column("subject", sqlalchemy.String(250)),
    sqlalchemy.Column("s_pic01", sqlalchemy.String(150)),
    sqlalchemy.Column("addinfo01", sqlalchemy.String(100)), 
    sqlalchemy.Column("addinfo02", sqlalchemy.String(100)),
    sqlalchemy.Column("addinfo12", sqlalchemy.String(250)),
    sqlalchemy.Column("addinfo15", sqlalchemy.String(250)),
    sqlalchemy.Column("addinfo03", sqlalchemy.String(10)),
    sqlalchemy.Column("addinfo04", sqlalchemy.String(10)),
    sqlalchemy.Column("addinfo05", sqlalchemy.String(10)),
    sqlalchemy.Column("addinfo07", sqlalchemy.String(10)),
    sqlalchemy.Column("addinfo08", sqlalchemy.Text),
    sqlalchemy.Column("addinfo09", sqlalchemy.Text),
    sqlalchemy.Column("addinfo10", sqlalchemy.Text),
    sqlalchemy.Column("addinfo11", sqlalchemy.String(250)),
    sqlalchemy.Column("addinfo19", sqlalchemy.String(250)),
)

sub02_table = sqlalchemy.Table(
    "homeprotectionsub02", metadata,
    sqlalchemy.Column("puid", sqlalchemy.Integer), 
    sqlalchemy.Column("s_pic01", sqlalchemy.String(150)),
    sqlalchemy.Column("num", sqlalchemy.Integer), 
)

# --- ë°ì´í„° ëª¨ë¸ ---
class Dog(BaseModel):
    uid: int
    subject: str
    s_pic01: Optional[str] = None
    image_filenames: List[str] = [] 
    addinfo01: Optional[str] = None 
    addinfo02: Optional[str] = None 
    addinfo03: Optional[str] = None
    addinfo04: Optional[str] = None
    addinfo05: Optional[str] = None
    addinfo07: Optional[str] = None
    addinfo08: Optional[str] = None
    addinfo09: Optional[str] = None
    addinfo10: Optional[str] = None
    addinfo11: Optional[str] = None
    addinfo12: Optional[str] = None
    addinfo15: Optional[str] = None
    addinfo19: Optional[str] = None

class RealProfileRequest(BaseModel):
    dog_uid: int

# --- ì•± ì´ˆê¸°í™” ---
models = {}
app = FastAPI()

# â­ï¸ [í•„ìˆ˜] CORS ì„¤ì • ì¶”ê°€ (ì—¬ê¸°!)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì£¼ì†Œ í—ˆìš© (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if torch.cuda.is_available():
    device = "cuda"
    gpu_id = 0
    print(f"ðŸš€ [System] GPU ëª¨ë“œ í™œì„±í™”: {torch.cuda.get_device_name(0)}")
else:
    device = "cpu"
    gpu_id = None
    print("âš ï¸ [System] ê²½ê³ : GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

SDXL_SERVICE_URL = "http://sdxl-service:8001/generate/background"

@app.on_event("startup")
def load_models_and_db():
    print("ðŸš€ AI ì„œë²„ ì‹œìž‘: ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # 1. Real-ESRGAN
    print("Loading Real-ESRGAN PyTorch Model...")
    try:
        model_arch = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
        model_path = "/app/esrgan/RealESRGAN_x4plus.pth"
        models["upsampler"] = RealESRGANer(
            scale=4, model_path=model_path, model=model_arch, tile=0, tile_pad=10, pre_pad=0,
            half=True if device == "cuda" else False, gpu_id=gpu_id
        )
        print("âœ… Real-ESRGAN Loaded.")
    except Exception as e:
        print(f"ðŸš¨ Real-ESRGAN Failed: {e}")

    # 2. Rembg
    print("Loading Rembg...")
    try:
        models["remover"] = new_session(model_name="isnet-general-use")
        print("âœ… Rembg Loaded.")
    except:
        models["remover"] = new_session()

    print("--- ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ ---")

@app.on_event("shutdown")
async def shutdown_db_client():
    if database and database.is_connected: await database.disconnect()

async def get_db_connection():
    if database and not database.is_connected: await database.connect()
    return database

# --- Helper Functions ---
def resize_image_if_too_large(img: Image.Image, max_dim: int = 1024) -> Image.Image:
    w, h = img.size
    if max(w, h) > max_dim:
        scale = max_dim / max(w, h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        return img.resize((new_w, new_h), Image.LANCZOS)
    return img

# â­ï¸ [í•µì‹¬ ìˆ˜ì •] "ì •ì‚¬ê°í˜• ìº”ë²„ìŠ¤"ì— ì´ë¯¸ì§€ë¥¼ "ê°€ìš´ë° ì •ë ¬"í•˜ëŠ” í•¨ìˆ˜
# ì´ë ‡ê²Œ í•˜ë©´ ê²‰ëª¨ì–‘ì€ ë¬´ì¡°ê±´ ì •ì‚¬ê°í˜•(ê¹”ë”í•¨)ì´ ë˜ê³ , ì‚¬ì§„ì€ ì•ˆ ìž˜ë¦½ë‹ˆë‹¤.
def create_framed_image(pil_img: Image.Image) -> Image.Image:
    w, h = pil_img.size
    
    # 1. ì •ì‚¬ê°í˜• ìº”ë²„ìŠ¤ í¬ê¸° ê²°ì • (ì‚¬ì§„ì˜ ê°€ìž¥ ê¸´ ë³€ ê¸°ì¤€)
    # ì¡°ê¸ˆ ë„‰ë„‰í•˜ê²Œ ìž¡ì•„ì„œ í•´ìƒë„ ìœ ì§€
    canvas_size = max(w, h)
    
    # 2. í°ìƒ‰ ì •ì‚¬ê°í˜• ìº”ë²„ìŠ¤ ìƒì„±
    square_canvas = Image.new('RGB', (canvas_size, canvas_size), 'white')
    
    # 3. ì¤‘ì•™ ì¢Œí‘œ ê³„ì‚°
    offset_x = (canvas_size - w) // 2
    offset_y = (canvas_size - h) // 2
    
    # 4. ì‚¬ì§„ ë¶™ì´ê¸° (ìž˜ë¦¼ ì—†ìŒ!)
    square_canvas.paste(pil_img, (offset_x, offset_y))
    
    # 5. í…Œë‘ë¦¬ ì¶”ê°€ (ìº”ë²„ìŠ¤ í¬ê¸°ì˜ 3% ì •ë„ë§Œ) - í´ë¼ë¡œì´ë“œ ëŠë‚Œ ì‚´ì§
    border_size = int(canvas_size * 0.03)
    framed_img = ImageOps.expand(square_canvas, border=border_size, fill='white')
    
    return framed_img

async def get_dog_details(dog_uid: int) -> Dog:
    db = await get_db_connection()
    if not db: raise HTTPException(status_code=500, detail="DB Fail")
    main_query = dogs_table.select().where(dogs_table.c.uid == dog_uid)
    dog_data = await db.fetch_one(main_query)
    if not dog_data: raise HTTPException(status_code=404, detail="Dog Not Found")
    image_query = sub02_table.select().where(sub02_table.c.puid == dog_uid).order_by(sub02_table.c.num)
    image_data_list = await db.fetch_all(image_query)
    image_filenames = [row['s_pic01'] for row in image_data_list]
    return Dog(**dog_data, image_filenames=image_filenames)

def pil_to_cv2(pil_image):
    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

def cv2_to_pil(cv2_image):
    return Image.fromarray(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))

def draw_text_with_stroke(draw, x, y, text, font, fill_color, stroke_color, stroke_width):
    for dx, dy in [(sx, sy) for sx in range(-stroke_width, stroke_width + 1) for sy in range(-stroke_width, stroke_width + 1) if sx * sx + sy * sy <= stroke_width * stroke_width]:
        draw.text((x + dx, y + dy), text, font=font, fill=stroke_color)
    draw.text((x, y), text, font=font, fill=fill_color)

def get_text_width(draw, text, font):
    try: return draw.textlength(text, font=font)
    except: return len(text) * (font.size * 0.6)

def remove_emojis(text):
    if not text: return ""
    return re.sub(r'[^\w\s,.\-?!@#%&()ê°€-íž£/]', '', text).strip()

async def call_sdxl_service(base64_dog_image: str, dog_info: dict) -> Image.Image:
    color_hint = "warm cream and white"
    prompt_detail = "A minimalist aesthetic background, warm sunlight shadows on a white wall, clean interior, cozy atmosphere, high quality, soft focus, instagram vibe."

    payload = {"base64_dog_image": base64_dog_image, "prompt": prompt_detail, "color_hint": color_hint}
    
    async with httpx.AsyncClient(timeout=100.0) as client:
        try:
            response = await client.post(SDXL_SERVICE_URL, json=payload)
            response.raise_for_status()  
            result = response.json()
            base64_bg = result.get("base64_background_image")
            if not base64_bg: raise ValueError("No bg image")
            return Image.open(io.BytesIO(base64.b64decode(base64_bg))).convert("RGB")
        except Exception as e:
            return Image.new('RGB', (1080, 1350), (250, 245, 240)) 

def generate_dog_text(dog: Dog) -> Dict: 
    def clean_text(text):
        if not text: return ""
        text = re.sub(r'<[^>]+>', '', text)
        return remove_emojis(text)

    raw_name = dog.subject.split('/')[0] if '/' in dog.subject else dog.subject
    dog_name_only = clean_text(raw_name).strip()
    display_age = dog.addinfo05 if dog.addinfo05 and not dog.addinfo05.isdigit() else "ì •ë³´ ì—†ìŒ"
    
    basic_info_lines = [
        f"ì´ë¦„: {dog_name_only}",
        f"ì„±ë³„: {clean_text(dog.addinfo03)}",
        f"ì¶œìƒì‹œê¸°: {display_age}", 
        f"ëª¸ë¬´ê²Œ: {clean_text(dog.addinfo07)}kg",
        f"ì¤‘ì„±í™”: {clean_text(dog.addinfo04)}",
    ]
    
    info_source = [dog.addinfo08, dog.addinfo09, dog.addinfo10, dog.addinfo01]
    story_data = f"ì´ë¦„:{dog_name_only}, " + " ".join([clean_text(x) for x in info_source if x])
    
    system_prompt = """
    ë‹¹ì‹ ì€ ìž…ì–‘ì„ ê¸°ë‹¤ë¦¬ëŠ” ìœ ê¸°ê²¬ìž…ë‹ˆë‹¤. ë¯¸ëž˜ì˜ ê°€ì¡±ì—ê²Œ ë³´ë‚´ëŠ” ì§§ì€ íŽ¸ì§€ë¥¼ ìž‘ì„±í•˜ì„¸ìš”.
    ê·œì¹™: 1ì¸ì¹­('ì €', 'ì œ'), ë‹¤ì •í•œ ì¡´ëŒ“ë§(í•´ìš”ì²´). 2~3ë¬¸ìž¥.
    """
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ê°•ì•„ì§€ ì •ë³´: {story_data}"}
    ]
    
    generated_story = ""
    try:
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        res = client.chat.completions.create(model="gpt-4o-mini", messages=messages, max_tokens=300)
        generated_story = remove_emojis(res.choices[0].message.content.strip())
    except Exception as e:
        generated_story = f"ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” {dog_name_only}ì˜ˆìš”! ì €ì˜ í‰ìƒ ê°€ì¡±ì´ ë˜ì–´ì£¼ì‹¤ ë¶„ì„ ê¸°ë‹¤ë¦¬ê³  ìžˆì–´ìš”."

    return {
        "basic_info": basic_info_lines,
        "story": generated_story,
        "name": dog_name_only
    }

async def select_best_image(dog: Dog) -> Union[Image.Image, None]:
    best_img, best_score = None, -9999
    imgs = list(dict.fromkeys([x for x in ([dog.s_pic01] + dog.image_filenames) if x and x.strip()]))
    if not imgs: return None
    
    remover = models.get("remover")
    if not remover: return None 

    print(f"[{dog.uid}] AI ìŠ¤ë§ˆíŠ¸ ì„ ë³„ ì¤‘ ({len(imgs)}ìž¥)...")
    
    for fname in imgs:
        try:
            url = f"{SITE_BASE_URL}{IMAGE_BASE_PATH}/{fname}"
            res = requests.get(url, stream=True, timeout=5)
            res.raise_for_status()
            img = Image.open(res.raw).convert("RGB")
            img = ImageOps.exif_transpose(img)
            
            w, h = img.size
            if w < 250 or h < 250: continue 

            small_w = 320
            small_h = int(h * (small_w / w))
            img_small = img.resize((small_w, small_h))
            
            no_bg = remove(img_small, session=remover, alpha_matting=False)
            
            alpha = np.array(no_bg.split()[3])
            if cv2.countNonZero(alpha) == 0: continue 
            
            coords = cv2.findNonZero(alpha)
            x, y, box_w, box_h = cv2.boundingRect(coords)
            
            mask_area = box_w * box_h
            total_area = small_w * small_h
            score_size = (mask_area / total_area) * 100 

            center_x = x + box_w / 2
            center_y = y + box_h / 2
            dist_from_center = ((center_x - small_w/2)**2 + (center_y - small_h/2)**2)**0.5
            max_dist = (small_w**2 + small_h**2)**0.5
            score_center = (1 - (dist_from_center / max_dist)) * 50 
            
            img_gray = cv2.cvtColor(np.array(img_small), cv2.COLOR_RGB2GRAY)
            masked_gray = img_gray[y:y+box_h, x:x+box_w]
            if masked_gray.size > 0:
                laplacian_var = cv2.Laplacian(masked_gray, cv2.CV_64F).var()
                score_sharp = min(laplacian_var, 500) / 10 
            else:
                score_sharp = 0
                
            score_penalty = 0
            if h > w * 2.2: score_penalty = 30 
            
            total_score = score_size + score_center + score_sharp - score_penalty
            if h > w: total_score += 20
            
            if total_score > best_score:
                best_score = total_score
                best_img = img
                
        except Exception as e:
            continue
            
    return best_img

# =========================================================
# 1. ìžë™ í”„ë¡œí•„ ìƒì„± (í•Œí”¼ë°”ì´ëŸ¬ìŠ¤ ê³µê³ )
# =========================================================
@app.post("/api/v1/generate-real-profile", response_model=dict)
async def generate_real_profile(request: RealProfileRequest):
    if "upsampler" not in models: raise HTTPException(status_code=503, detail="Model Loading")
    try:
        dog = await get_dog_details(request.dog_uid)
        best_img = await select_best_image(dog)
        if not best_img: return {"profile_text": "ì´ë¯¸ì§€ ì—†ìŒ", "profile_image_base64": ""}
        
        best_img = resize_image_if_too_large(best_img)

        cv2_img = pil_to_cv2(best_img)
        output, _ = models["upsampler"].enhance(cv2_img, outscale=4)
        upscaled_pil = cv2_to_pil(output)

        # â­ï¸ [ì ìš©] ì •ì‚¬ê°í˜• ì•¡ìžì— ì´ë¯¸ì§€ ì¤‘ì•™ ë°°ì¹˜ (Fit)
        processed_img = create_framed_image(upscaled_pil)
        
        buf = io.BytesIO()
        processed_img.save(buf, format="PNG") 
        b64_png = base64.b64encode(buf.getvalue()).decode("utf-8")
        bg_img = await call_sdxl_service(b64_png, {"name": dog.subject})
        
        text_data = generate_dog_text(dog)
        
        template_w, template_h = 1080, 1350
        template = bg_img.resize((template_w, template_h))
        draw = ImageDraw.Draw(template)
        
        try:
            ft = ImageFont.truetype("/app/KyoboHandwriting2021sjy.otf", 80)
            fb = ImageFont.truetype("/app/KyoboHandwriting2021sjy.otf", 38)
        except: 
            ft = fb = ImageFont.load_default()

        t_txt = f"{text_data['name']}ì˜ ê°€ì¡±ì„ ì°¾ìŠµë‹ˆë‹¤."
        tw = get_text_width(draw, t_txt, ft)
        draw_text_with_stroke(draw, (template_w-tw)/2, 60, t_txt, ft, (255,255,255), (0,0,0), 3)
        header_height = 180

        lines = text_data['basic_info'] + textwrap.wrap(text_data['story'], width=35)
        line_height = 50
        text_total_height = (len(lines) * line_height) + 50 
        footer_margin = 100 
        
        available_h = template_h - header_height - text_total_height - footer_margin
        
        p_w, p_h = processed_img.size
        # ê°€ë¡œ 900ìœ¼ë¡œ ê³ ì • (ì •ì‚¬ê°í˜•ì´ë¯€ë¡œ ì„¸ë¡œë„ 900ì´ ë¨)
        target_w = 900
        target_h = int(p_h * (target_w / p_w))
        
        if target_h > available_h:
            target_h = available_h
            target_w = int(p_w * (target_h / p_h))

        paste_img = processed_img.resize((target_w, target_h))
        template.paste(paste_img, ((template_w-target_w)//2, header_height))
        
        cy = header_height + target_h + 40
        for line in lines:
            w = get_text_width(draw, line, fb)
            draw_text_with_stroke(draw, (template_w-w)/2, cy, line, fb, (50,50,50), (255,255,255), 2)
            cy += line_height
        
        buf = io.BytesIO()
        template = template.convert("RGB")
        template.save(buf, format="JPEG", quality=90, optimize=True)
        final_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
        
        torch.cuda.empty_cache()
        
        return {
            "profile_text": '\n'.join(text_data['basic_info'] + [text_data['story']]), 
            "profile_image_base64": final_b64
        }
        
    except Exception as e:
        print(f"ðŸš¨ Auto Profile Error: {e}")
        return {"profile_text": "Error", "profile_image_base64": ""}

# =========================================================
# 2. ìž…ì–‘/ìž„ë³´ í”„ë¡œí•„ (ìˆ˜ë™ ìž…ë ¥)
# =========================================================
@app.post("/api/v1/generate-adoption-profile", response_model=dict)
async def generate_adoption_profile(
    image: UploadFile = File(...),
    name: str = Form(...),
    age: str = Form(...),
    personality: str = Form(...),
    features: str = Form(...)
):
    if "upsampler" not in models: raise HTTPException(status_code=503, detail="Model Loading")

    try:
        contents = await image.read()
        if len(contents) == 0: raise ValueError("Uploaded file is empty.")

        img = Image.open(io.BytesIO(contents)).convert("RGB")
        img = ImageOps.exif_transpose(img) 
        img = resize_image_if_too_large(img)

        cv2_img = pil_to_cv2(img)
        output, _ = models["upsampler"].enhance(cv2_img, outscale=4)
        upscaled_pil = cv2_to_pil(output)

        # â­ï¸ [ì ìš©] ì •ì‚¬ê°í˜• ì•¡ìžì— ì´ë¯¸ì§€ ì¤‘ì•™ ë°°ì¹˜ (Fit)
        processed_img = create_framed_image(upscaled_pil)
        
        buf = io.BytesIO()
        processed_img.save(buf, format="PNG")
        b64_png = base64.b64encode(buf.getvalue()).decode("utf-8")
        bg_img = await call_sdxl_service(b64_png, {"name": name})

        # í”„ë¡¬í”„íŠ¸: ì„±ê²©/íŠ¹ì§•ì„ ë…¹ì—¬ë‚´ë„ë¡ ìš”ì²­
        story_data = f"ì´ë¦„:{name}, ë‚˜ì´:{age}, ì„±ê²©:{personality}, íŠ¹ì§•:{features}"
        system_prompt = """
        ë‹¹ì‹ ì€ ìž…ì–‘ì„ ê¸°ë‹¤ë¦¬ëŠ” ê°•ì•„ì§€ìž…ë‹ˆë‹¤. ë¯¸ëž˜ì˜ ê°€ì¡±ì—ê²Œ ë³´ë‚´ëŠ” íŽ¸ì§€ë¥¼ ìž‘ì„±í•˜ì„¸ìš”.
        ê·œì¹™:
        1. ì œê³µëœ ì„±ê²©ê³¼ íŠ¹ì§•ì„ **ëª¨ë‘ ìžì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ì„œ** í•˜ë‚˜ì˜ ì´ì•¼ê¸°ë¡œ ë§Œë“œì„¸ìš”.
        2. ì‹œì ì€ 'ì €', 'ì œ'ë¥¼ ì‚¬ìš©í•œ 1ì¸ì¹­ ì‹œì ìž…ë‹ˆë‹¤.
        3. ë§íˆ¬ëŠ” ì‚¬ëž‘ìŠ¤ëŸ½ê³  ë‹¤ì •í•œ 'í•´ìš”ì²´' ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        4. ê¸¸ì´ëŠ” 2~3ë¬¸ìž¥ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ë‚´ ì •ë³´: {story_data}"}
        ]
        
        try:
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            res = client.chat.completions.create(model="gpt-4o-mini", messages=messages, max_tokens=300)
            generated_story = remove_emojis(res.choices[0].message.content.strip())
        except:
            generated_story = f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” {name}ì´ì—ìš”. ì‚¬ëž‘ ë„˜ì¹˜ëŠ” ê°€ì¡±ì„ ê¸°ë‹¤ë¦¬ê³  ìžˆì–´ìš”!"

        template_w, template_h = 1080, 1350
        template = bg_img.resize((template_w, template_h))
        draw = ImageDraw.Draw(template)
        
        try:
            ft = ImageFont.truetype("/app/KyoboHandwriting2021sjy.otf", 80)
            fb = ImageFont.truetype("/app/KyoboHandwriting2021sjy.otf", 38)
        except: ft = fb = ImageFont.load_default()

        t_txt = f"{name}ì˜ ê°€ì¡±ì„ ì°¾ìŠµë‹ˆë‹¤."
        tw = get_text_width(draw, t_txt, ft)
        draw_text_with_stroke(draw, (template_w-tw)/2, 60, t_txt, ft, (255,255,255), (0,0,0), 3)
        
        # â­ï¸ [í•µì‹¬ ìˆ˜ì •] ì„±ê²©/íŠ¹ì§• ë‚ ê²ƒ ì¶œë ¥ ì‚­ì œ! ì´ë¦„/ë‚˜ì´ë§Œ ì¶œë ¥
        info_lines = [f"ì´ë¦„: {name}", f"ë‚˜ì´: {age}"]
        
        # AI ìŠ¤í† ë¦¬ë§Œ ë¶™ìž„
        lines = info_lines + textwrap.wrap(generated_story, width=35)
        
        header_height = 180
        line_height = 50
        text_total_height = (len(lines) * line_height) + 50
        footer_margin = 100
        
        available_h = template_h - header_height - text_total_height - footer_margin
        p_w, p_h = processed_img.size
        
        # ê°€ë¡œ 900 ê³ ì • (ì •ì‚¬ê°í˜•ì´ë¯€ë¡œ)
        target_w = 900
        target_h = int(p_h * (target_w / p_w))
        
        if target_h > available_h:
            target_h = available_h
            target_w = int(p_w * (target_h / p_h))

        paste_img = processed_img.resize((target_w, target_h))
        template.paste(paste_img, ((template_w-target_w)//2, header_height))
        
        cy = header_height + target_h + 40
        for line in lines:
            w = get_text_width(draw, line, fb)
            draw_text_with_stroke(draw, (template_w-w)/2, cy, line, fb, (50,50,50), (255,255,255), 2)
            cy += line_height

        buf = io.BytesIO()
        template = template.convert("RGB")
        template.save(buf, format="JPEG", quality=90, optimize=True)
        final_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
        
        torch.cuda.empty_cache()

        # ë°˜í™˜ê°’ë„ ì •ë¦¬
        return {"profile_text": '\n'.join(info_lines + [generated_story]), "profile_image_base64": final_b64}

    except Exception as e:
        print(f"ðŸš¨ Adoption Profile Error: {e}")
        raise HTTPException(status_code=422, detail="Unsupported Image Format or Corrupt File.")

# =========================================================
# 3. ìŠ¤íŠœë””ì˜¤ í”„ë¡œí•„ (ëˆ„ë¼ + ì¤‘ì•™ ì •ë ¬ + HEIC ì§€ì›)
# =========================================================
@app.post("/api/v1/generate-studio-profile", response_model=dict)
async def generate_studio_profile(
    image: UploadFile = File(...),
    bg_color: str = Form("#FFD1DC") 
):
    if "upsampler" not in models or "remover" not in models: 
        raise HTTPException(status_code=503, detail="Model Loading")

    try:
        contents = await image.read()
        if len(contents) == 0: raise ValueError("Uploaded file is empty.")

        img = Image.open(io.BytesIO(contents)).convert("RGB")
        img = ImageOps.exif_transpose(img)
        
        w, h = img.size
        
        TARGET_SIZE = 1280
        if max(w, h) > TARGET_SIZE:
            scale = TARGET_SIZE / max(w, h)
            new_w, new_h = int(w * scale), int(h * scale)
            img = img.resize((new_w, new_h), Image.LANCZOS)
        
        if max(w, h) < 1000:
            cv2_img = pil_to_cv2(img)
            output, _ = models["upsampler"].enhance(cv2_img, outscale=4)
            img = cv2_to_pil(output)
            img = resize_image_if_too_large(img, max_dim=1500)

        no_bg = remove(
            img, 
            session=models["remover"], 
            alpha_matting=True,
            alpha_matting_foreground_threshold=240,
            alpha_matting_background_threshold=10,
            alpha_matting_erode_size=10
        )

        bbox = no_bg.getbbox() 
        if bbox:
            subject_only = no_bg.crop(bbox) 
        else:
            subject_only = no_bg 

        TARGET_W, TARGET_H = 1080, 1350
        try:
            color_rgb = ImageColor.getrgb(bg_color)
        except:
            color_rgb = (255, 240, 245)
        final_canvas = Image.new("RGB", (TARGET_W, TARGET_H), color_rgb)

        MAX_SUB_W = int(TARGET_W * 0.9)
        MAX_SUB_H = int(TARGET_H * 0.9)

        s_w, s_h = subject_only.size
        scale_w = MAX_SUB_W / s_w
        scale_h = MAX_SUB_H / s_h
        scale_factor = min(scale_w, scale_h) 

        new_s_w = int(s_w * scale_factor)
        new_s_h = int(s_h * scale_factor)
        resized_subject = subject_only.resize((new_s_w, new_s_h), Image.LANCZOS)

        paste_x = (TARGET_W - new_s_w) // 2
        paste_y = (TARGET_H - new_s_h) // 2
        final_canvas.paste(resized_subject, (paste_x, paste_y), resized_subject)

        buf = io.BytesIO()
        final_canvas.save(buf, format="JPEG", quality=90, optimize=True)
        final_b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
        
        torch.cuda.empty_cache()

        return {"profile_image_base64": final_b64, "message": "ì„±ê³µ"}

    except Exception as e:
        print(f"ðŸš¨ Studio Profile Error: {e}")
        return {"profile_image_base64": "", "message": "Error"}
